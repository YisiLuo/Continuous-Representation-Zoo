# Continuous-Representation-Zoo
This project summarizes the papers and studies introduced in the review: Continuous Representation Methods, Theories, and Applications: An Overview and Perspectives ğŸ“–
ZGC
# Awesome-Multimodal-Large-Language-Models

<p align="center">
    <img src="./images/MiG_logo.jpg" width="100%" height="100%">
</p>
INR
## Our MLLM works

ğŸ”¥ğŸ”¥ğŸ”¥ **A Survey on Multimodal Large Language Models**  
**[Project Page [This Page]](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)** | **[Paper](https://arxiv.org/pdf/2306.13549.pdf)** | :black_nib: **[Citation](./images/bib_survey.txt)** | **[ğŸ’¬ WeChat (MLLMå¾®ä¿¡äº¤æµç¾¤ï¼Œæ¬¢è¿åŠ å…¥)](./images/wechat-group.png)**

The first comprehensive survey for Multimodal Large Language Models (MLLMs). :sparkles:  

---

ğŸ”¥ğŸ”¥ğŸ”¥ **VITA: Towards Open-Source Interactive Omni Multimodal LLM**  
<p align="center">
    <img src="./images/vita-1.5.jpg" width="60%" height="60%">
</p>

<font size=7><div align='center' > [[ğŸ“½ VITA-1.5 Demo Show! Here We Go! ğŸ”¥](https://youtu.be/tyi6SVFT5mM?si=fkMQCrwa5fVnmEe7)] </div></font>  

<font size=7><div align='center' > [[ğŸ“– VITA-1.5 Paper](https://arxiv.org/pdf/2501.01957)] [[ğŸŒŸ GitHub](https://github.com/VITA-MLLM/VITA)] [[ğŸ¤– Basic Demo](https://modelscope.cn/studios/modelscope/VITA1.5_demo)] [[ğŸ VITA-1.0](https://vita-home.github.io/)] [[ğŸ’¬ WeChat (å¾®ä¿¡)](https://github.com/VITA-MLLM/VITA/blob/main/asset/wechat-group.jpg)]</div></font>  

<font size=7><div align='center' > We are excited to introduce the **VITA-1.5**, a more powerful and more real-time version. âœ¨ </div></font>

<font size=7><div align='center' >**All codes of VITA-1.5 have been released**! :star2: </div></font>  

You can experience our [Basic Demo](https://modelscope.cn/studios/modelscope/VITA1.5_demo) on ModelScope directly. The Real-Time Interactive Demo needs to be configured according to the [instructions](https://github.com/VITA-MLLM/VITA?tab=readme-ov-file#-real-time-interactive-demo).


---

ğŸ”¥ğŸ”¥ğŸ”¥ **Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuracy**  
<p align="center">
    <img src="./images/longvita.jpg" width="80%" height="80%">
</p>

<font size=7><div align='center' > [[ğŸ“– arXiv Paper](https://arxiv.org/pdf/2502.05177)] [[ğŸŒŸ GitHub](https://github.com/VITA-MLLM/Long-VITA)]</div></font>  

<font size=7><div align='center' > Process more than **4K frames** or over **1M visual tokens**. State-of-the-art on Video-MME under 20B models!  âœ¨ </div></font>


---

ğŸ”¥ğŸ”¥ğŸ”¥ **MM-RLHF: The Next Step Forward in Multimodal LLM Alignment**  
<p align="center">
    <img src="./images/mm-rlhf.jpg" width="60%" height="60%">
</p>

<font size=7><div align='center' > [[ğŸ“– arXiv Paper](https://arxiv.org/pdf/2502.10391)] [[ğŸŒŸ GitHub](https://github.com/Kwai-YuanQi/MM-RLHF)] [[ğŸ“Š MM-RLHF Data](https://huggingface.co/datasets/yifanzhang114/MM-RLHF)] </div></font>  

Align MLLMs with human preference, including a high-quality dataset, a strong reward model, a new alignmen algorithm, and two new benchmarks.âœ¨


---

ğŸ”¥ğŸ”¥ğŸ”¥ **MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs**  
<p align="center">
    <img src="./images/mme-survey.jpg" width="60%" height="60%">
</p>

<font size=7><div align='center' > [[ğŸ Project Page](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Benchmarks)] [[ğŸ“– arXiv Paper](https://arxiv.org/pdf/2411.15296)] </div></font>

<font size=7><div align='center' > Jointly introduced by **MME**, **MMBench**, and **LLaVA** teams. âœ¨ </div></font>

---

ğŸ”¥ğŸ”¥ğŸ”¥ **Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis**  
**[Project Page](https://video-mme.github.io/)** | **[Paper](https://arxiv.org/pdf/2405.21075)** | **[GitHub](https://github.com/BradyFU/Video-MME)** | **[Dataset](https://github.com/BradyFU/Video-MME?tab=readme-ov-file#-dataset)** | **[Leaderboard](https://video-mme.github.io/home_page.html#leaderboard)**

We are very proud to launch Video-MME, the first-ever comprehensive evaluation benchmark of MLLMs in Video Analysis! ğŸŒŸ  

It includes short- (< 2min), medium- (4min\~15min), and long-term (30min\~60min) videos, ranging from <b>11 seconds to 1 hour</b>. All data are newly collected and annotated by humans, not from any existing video dataset. âœ¨ 

---

ğŸ”¥ğŸ”¥ğŸ”¥ **MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models**  
**[Paper](https://arxiv.org/pdf/2306.13394.pdf)** | **[Download](https://huggingface.co/datasets/darkyarding/MME/blob/main/MME_Benchmark_release_version.zip)** | **[Eval Tool](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/blob/Evaluation/tools/eval_tool.zip)** | :black_nib: **[Citation](./images/bib_mme.txt)**

A representative evaluation benchmark for MLLMs. :sparkles:  

---

ğŸ”¥ğŸ”¥ğŸ”¥ **Woodpecker: Hallucination Correction for Multimodal Large Language Models**  
**[Paper](https://arxiv.org/pdf/2310.16045)** | **[GitHub](https://github.com/BradyFU/Woodpecker)**

This is the first work to correct hallucination in multimodal large language models. :sparkles:  

