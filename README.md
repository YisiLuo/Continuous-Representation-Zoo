# Continuous-Representation-Zoo

This project summarizes the papers and studies introduced in the review: Continuous Representation Methods, Theories, and Applications: An Overview and Perspectives ðŸ“–

# Abstract âœ¨

Recently, continuous representation methods emerge as novel paradigms that characterize the intrinsic structures of real-world data through function representations that map positional coordinates to their corresponding values in the continuous space. As compared with the traditional discrete framework, the continuous framework demonstrates inherent superiority for data representation and reconstruction (e.g., image restoration, novel view synthesis, and waveform inversion) by offering inherent advantages including resolution flexibility, cross-modal adaptability, inherent smoothness, and parameter efficiency. In this review, we systematically examine recent advancements in continuous representation frameworks, focusing on three aspects: (i) Continuous representation method designs such as basis function representation, statistical modeling, tensor function decomposition, and implicit neural representation; (ii) Theoretical foundations of continuous representations such as approximation error analysis, convergence property, and implicit regularization; (iii) Real-world applications of continuous representations derived from computer vision, graphics, bioinformatics, and remote sensing. Furthermore, we outline future directions and perspectives to inspire exploration and deepen insights to facilitate continuous representation methods, theories, and applications.

<p align="center">
    <img src="imgs/diagram.png" width="100%">
</p>

    bibtex:
    @article{arXiv2025Luo,
     author  = {Yisi Luo, Xile Zhao, Deyu Meng},
     title   = {Continuous Representation Methods, Theories, and Applications: An Overview and Perspectives},
     journal = {arXiv:},
     year    = {2025},
    }


# Continuous Methods (Parametric model)

## Basis Function Representation

- Tatsuya Yokota, Rafal Zdunek, Andrzej Cichocki, and Yukihiko Yamashita. Smooth nonnegative matrix and tensor factorizations for robust multi-way data analysis. Signal Processing, 2015. [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

- Otto Debals, Marc Van Barel, and Lieven De Lathauwer. Nonnegative matrix factorization using nonnegative polynomial approximations. IEEE Signal Processing Letters, 2017 [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

- Masaaki Imaizumi and Kohei Hayashi. Tensor decomposition with smoothness. In International Conference on Machine
Learning, 2017. [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

- Alex A. Gorodetsky and John D. Jakeman. Gradient-based optimization for regression in the functional tensor-train format.
Journal of Computational Physics, 2018. [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

- Nikos Kargas and Nicholas D. Sidiropoulos. Nonlinear system identiffcation via tensor completion. In Proceedings of the
AAAI Conference on Artificial Intelligence, 2020. [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

- Nikos Kargas and Nicholas D. Sidiropoulos. Supervised learning and canonical decomposition of multivariate functions. IEEE Transactions on Signal Processing, 2021. [<a href="https://ieeexplore.ieee.org/document/9340610">paper</a>]

- Lucas Sort, Laurent Le Brusquet, and Arthur Tenenhaus. Latent functional parafac for modeling multidimensional longitudinal
data. arXiv, 2410.18696, 2024. [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

- Peter Kunkel and Volker Mehrmann. Smooth factorizations of matrix-valued functions and their derivatives. Numerische Mathematik, 1991. [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

- I. V. Oseledets. Constructive representation of functions in low-rank tensor formats. Constructive Approximation, 2013. [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

- Petr Tichavsky and Ondrej Straka. Tensor train approximation of multivariate functions. In 2024 32nd European Signal
Processing Conference, 2024. [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

- Tianqi Chen, Hang Li, Qiang Yang, and Yong Yu. General functional matrix factorization using gradient boosting. In Proceedings of the 30th International Conference on International Conference on Machine Learning, 2013. [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

- Behnam Hashemi and Lloyd N. Trefethen. Chebfun in three dimensions. SIAM Journal on Scientic Computing, 2017. [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

- Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljacic, Thomas Y. Hou, and Max Tegmark. KAN: Kolmogorov-Arnold networks. In The Thirteenth International Conference on Learning Representations, 2025. [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

- Tongle Wu and Jicong Fan. Smooth tensor product for tensor completion. IEEE Transactions on Image Processing, 2024. [<a href="https://www.sciencedirect.com/science/article/pii/S0165168415000614">paper</a>]

## Implicit Neural Representation

## Grid Encoding Parametric Model

<p align="center">
    <img src="imgs/method1.png" width="100%">
</p>

# Continuous Methods (Structural modeling)

<p align="center">
    <img src="imgs/method2.png" width="100%">
</p>

# Theoretical Foundations

<p align="center">
    <img src="imgs/theory.png" width="100%">
</p>

# Applications

<p align="center">
    <img src="imgs/application.png" width="100%">
</p>


